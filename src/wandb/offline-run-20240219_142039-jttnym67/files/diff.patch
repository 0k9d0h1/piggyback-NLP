diff --git a/src/__pycache__/dataset.cpython-310.pyc b/src/__pycache__/dataset.cpython-310.pyc
index 1c2d780..42ddba6 100644
Binary files a/src/__pycache__/dataset.cpython-310.pyc and b/src/__pycache__/dataset.cpython-310.pyc differ
diff --git a/src/asdf.py b/src/asdf.py
index 40e61c5..5fdef78 100644
--- a/src/asdf.py
+++ b/src/asdf.py
@@ -1,5 +1,54 @@
+import avalanche
 import torch
+import itertools
+import pandas as pd
+import random
+import json
+import jsonlines
+import xml.etree.ElementTree as ET
+import argparse
+import torch.nn as nn
+import avalanche.evaluation.metrics as amet
 
-a = torch.rand(2, 768, 512)
+from avalanche.benchmarks.generators import tensors_benchmark, dataset_benchmark
+from avalanche.training.templates import SupervisedTemplate
+from avalanche.logging import InteractiveLogger, WandBLogger
+from avalanche.training.plugins import EvaluationPlugin
+from transformers import BertModel
+from transformers import AutoTokenizer
+from transformers import BertConfig
+from modnets import BertForPreTraining
+from datasets import Dataset
+from torch.utils.data import TensorDataset
+from dataset import create_unlabeled_benchmark, create_endtask_benchmark
+from typing import Any, Callable, Dict, Iterable, List, NamedTuple, Optional, Sequence, Union
+import multiprocessing
 
-print(a[:, None, :, :].shape)
+
+FLAGS = argparse.ArgumentParser()
+FLAGS.add_argument('--datasets', type=str, default='realnews',
+                   help='Names of datasets')
+FLAGS.add_argument('--model', type=str, default='bert-base-uncased',
+                   help='type of pretrained model')
+FLAGS.add_argument('--epoch', type=int, default=5)
+FLAGS.add_argument('--seed', type=int, default=1)
+args = FLAGS.parse_args()
+
+tokenizer = AutoTokenizer.from_pretrained(args.model)
+benchmark_unlabeled = create_unlabeled_benchmark(args, tokenizer)
+# a = 0
+# with jsonlines.open('../data/unlabeled/realnews/realnews.jsonl') as f:
+#     for i, lin in (enumerate(f)):
+#         print(i)
+
+# FLAGS = argparse.ArgumentParser()
+# FLAGS.add_argument('--datasets', type=str, default='pubmed',
+#                    help='Names of datasets')
+# FLAGS.add_argument('--model', type=str, default='bert-base-uncased',
+#                    help='type of pretrained model')
+# FLAGS.add_argument('--epoch', type=int, default=5)
+# FLAGS.add_argument('--seed', type=int, default=1)
+# args = FLAGS.parse_args()
+
+# tokenizer = AutoTokenizer.from_pretrained(args.model)
+# benchmark_unlabeled = create_endtask_benchmark(args, tokenizer)
diff --git a/src/dataset.py b/src/dataset.py
index f803844..b5e76be 100644
--- a/src/dataset.py
+++ b/src/dataset.py
@@ -16,7 +16,7 @@ MASK_PROP = 0.15
 def get_nsp_data(tokenizer, paragraphs):
     label = []
     nsp_data = []
-    for paragraph in paragraphs:
+    for paragraph in tqdm(paragraphs):
         for i in range(len(paragraph) - 1):
             if random.random() < 0.5:
                 label.append(1)
@@ -32,8 +32,10 @@ def get_nsp_data(tokenizer, paragraphs):
 
 def mask_nsp_data(tokenizer, paragraphs):
     nsp_data, label = get_nsp_data(tokenizer, paragraphs)
+    original_input_ids = torch.stack(
+        [torch.Tensor(a['input_ids']) for a in nsp_data])
 
-    for i, tokens in enumerate(nsp_data):
+    for i, tokens in tqdm(enumerate(nsp_data)):
         for j, token in enumerate(tokens['input_ids']):
             if token != 0 and token != 101 and token != 102:
                 if random.random() < MASK_PROP:
@@ -41,16 +43,17 @@ def mask_nsp_data(tokenizer, paragraphs):
                         nsp_data[i]['input_ids'][j] = 103
                     else:
                         if random.random() < 0.5:
-                            nsp_data[i]['input_ids'][j] = random.randint(
-                                1, len(tokenizer.get_vocab()))
-    return nsp_data, label
+                            r = random.randint(
+                                1000, len(tokenizer.get_vocab()) - 1)
+                            nsp_data[i]['input_ids'][j] = r
+    return nsp_data, original_input_ids, label
 
 
 def get_paragraphs(data):
     paragraphs = []
     if data == 'realnews':
         with jsonlines.open('../data/unlabeled/realnews/realnews.jsonl') as f:
-            for i, lin in enumerate(f):
+            for i, lin in tqdm(enumerate(f)):
                 sentences = lin['text'].strip().replace(
                     '\n', '').lower().split('. ')
                 paragraph = []
@@ -59,13 +62,11 @@ def get_paragraphs(data):
                         paragraph.append(sentence)
                 if len(paragraph) != 0:
                     paragraphs.append(paragraph)
-                if i == 2:
-                    break
 
     elif data == 'pubmed':
         pubmed = load_dataset('pubmed', streaming=True, trust_remote_code=True)
         i = 0
-        for entry in pubmed['train']:
+        for entry in tqdm(pubmed['train']):
             abstracttext = entry['MedlineCitation']['Article']['Abstract']['AbstractText']
             sentences = abstracttext.strip().replace('\n', '').lower().split('. ')
             paragraph = []
@@ -75,12 +76,10 @@ def get_paragraphs(data):
                     i += 1
             if len(paragraph) != 0:
                 paragraphs.append(paragraph)
-            if i > 10:
-                break
 
     elif data == 's2orc':
         file_list = os.listdir('../data/unlabeled/s2orc')
-        for file in file_list:
+        for file in tqdm(file_list):
             with jsonlines.open('../data/unlabeled/s2orc/%s' % file) as f:
                 i = 0
                 for lin in f:
@@ -94,8 +93,6 @@ def get_paragraphs(data):
                                 paragraph.append(sentence)
                         if len(paragraph) != 0:
                             paragraphs.append(paragraph)
-                        if i == 2:
-                            break
     return paragraphs
 
 
@@ -103,267 +100,277 @@ def create_unlabeled_benchmark(args, tokenizer):
     dat = args.datasets.split(',')
     train_datasets = []
     test_datasets = []
+    val_datasets = []
 
     for task_label, dataset in enumerate(dat):
-        paragraphs = get_paragraphs(dataset)
-        nsp_data, label = mask_nsp_data(tokenizer, paragraphs)
+        path = "../datasets/%s/unlabeled" % (dataset)
+        dir = os.listdir(path)
+        if len(dir) == 0:
+            paragraphs = get_paragraphs(dataset)
+            nsp_data, original_input_ids, label = mask_nsp_data(
+                tokenizer, paragraphs)
+
+            input_ids = torch.stack(
+                [torch.Tensor(a['input_ids']).type(torch.IntTensor) for a in nsp_data])
+            token_type_ids = torch.stack(
+                [torch.Tensor(a['token_type_ids']).type(torch.IntTensor) for a in nsp_data])
+            attention_mask = torch.stack(
+                [torch.Tensor(a['attention_mask']).type(torch.IntTensor) for a in nsp_data])
+            label = torch.Tensor(label)
+            length = label.shape[0]
+
+            nsp_data = torch.stack(
+                [input_ids, original_input_ids, token_type_ids, attention_mask], dim=1)
+
+            idx = torch.randperm(length)
+            nsp_data = nsp_data[idx].view(nsp_data.size())
+            label = label[idx].view(label.size())
+
+            train_dataset = make_classification_dataset(
+                TensorDataset(nsp_data[:int(length * 4 / 5)], label[:int(length * 4 / 5)]), task_labels=task_label)
+            val_dataset = make_classification_dataset(
+                TensorDataset(nsp_data[int(length * 4 / 5):int(length * 9 / 10)], label[int(length * 4 / 5):int(length * 9 / 10)]), task_labels=task_label)
+            test_dataset = make_classification_dataset(
+                TensorDataset(nsp_data[int(length * 9 / 10):], label[int(length * 9 / 10):]), task_labels=task_label)
+
+            torch.save(train_dataset, "%s/train.pt" % (path))
+            torch.save(val_dataset, "%s/val.pt" % (path))
+            torch.save(test_dataset, "%s/test.pt" % (path))
+
+        else:
+            train_dataset = torch.load("%s/train.pt" % (path))
+            val_dataset = torch.load("%s/val.pt" % (path))
+            test_dataset = torch.load("%s/test.pt" % (path))
+
+        train_datasets.append(train_dataset)
+        val_datasets.append(val_dataset)
+        test_datasets.append(test_dataset)
+
+    return dataset_benchmark(train_datasets, test_datasets, other_streams_datasets={"val": val_datasets})
+
+
+def preprocess_hyperpartisan(lin, tokenizer):
+    text = re.sub('<[^>]+>', '', lin['text'])
+    index = text.find('&#')
+    text = text.replace("..", "")
+    while index > -1:
+        idx_semicolon = index + 2
+        if index + 2 < len(text) and text[index+2].isdigit():
+            while idx_semicolon < len(text) and text[idx_semicolon] != ';':
+                idx_semicolon += 1
+                if index - idx_semicolon > 6:
+                    break
+            if text[index+2:idx_semicolon].isdigit():
+                unicode_decimal = int(text[index+2:idx_semicolon])
+                unicode = f'&#%d;' % unicode_decimal
+                text = text.replace(unicode,
+                                    chr(unicode_decimal))
+        index = text.find('&#')
+    text = text.replace("&amp;#160", " ")
+    text = text.replace("&amp;amp;", "&")
+    text = text.replace("&amp;gt;", ">")
 
-        input_ids = torch.stack(
-            [torch.Tensor(a['input_ids']) for a in nsp_data])
-        token_type_ids = torch.stack(
-            [torch.Tensor(a['token_type_ids']) for a in nsp_data])
-        attention_mask = torch.stack(
-            [torch.Tensor(a['attention_mask']) for a in nsp_data])
-        label = torch.Tensor(label)
-        length = label.shape[0]
+    tokens = tokenizer(text, padding='max_length',
+                       max_length=512, truncation='longest_first')
+    input_ids = torch.Tensor(tokens['input_ids'])
+    token_type_ids = torch.Tensor(tokens['token_type_ids'])
+    attention_mask = torch.Tensor(tokens['attention_mask'])
 
-        nsp_data = torch.stack(
-            [input_ids, token_type_ids, attention_mask], dim=1)
+    nsp_data = torch.stack(
+        [input_ids, token_type_ids, attention_mask], dim=0)
 
-        idx = torch.randperm(length)
-        nsp_data = nsp_data[idx].view(nsp_data.size())
-        label = label[idx].view(label.size())
+    return nsp_data
 
-        train_datasets.append(make_classification_dataset(
-            TensorDataset(nsp_data[:int(length * 4 / 5)], label[:int(length * 4 / 5)]), task_labels=task_label))
-        test_datasets.append(make_classification_dataset(
-            TensorDataset(nsp_data[int(length * 4 / 5):], label[int(length * 4 / 5):]), task_labels=task_label))
 
-    return dataset_benchmark(train_datasets, test_datasets)
+def preprocess(lin, tokenizer):
+    tokens = tokenizer(lin['text'], padding='max_length',
+                       max_length=512, truncation='longest_first')
+    input_ids = torch.Tensor(tokens['input_ids'])
+    token_type_ids = torch.Tensor(tokens['token_type_ids'])
+    attention_mask = torch.Tensor(tokens['attention_mask'])
+
+    nsp_data = torch.stack(
+        [input_ids, token_type_ids, attention_mask], dim=0)
+
+    return nsp_data
 
 
 def create_endtask_benchmark(args, tokenizer):
     dat = args.datasets.split(',')
     train_datasets = []
     test_datasets = []
+    val_datasets = []
 
     for task_label, dataset in enumerate(dat):
+        path = "../datasets/%s/endtask" % (dataset)
+        dir = os.listdir(path)
         if dataset == 'realnews':
-            hyperpartisan = load_dataset('hyperpartisan_news_detection', 'bypublisher',
-                                         trust_remote_code=True)
-            str_to_idx = {'false': 0,
-                          'true': 1}
-
-            label_train = []
-            input_train = []
-            for i, lin in enumerate(hyperpartisan['train']):
-                text = re.sub('<[^>]+>', '', lin['text'])
-                index = text.find('&#')
-                text = text.replace("..", "")
-                while index > -1:
-                    idx_semicolon = index + 2
-                    if index + 2 < len(text) and text[index+2].isdigit():
-                        while idx_semicolon < len(text) and text[idx_semicolon] != ';':
-                            idx_semicolon += 1
-                            if index - idx_semicolon > 6:
-                                break
-                        if text[index+2:idx_semicolon].isdigit():
-                            unicode_decimal = int(text[index+2:idx_semicolon])
-                            unicode = f'&#%d;' % unicode_decimal
-                            text = text.replace(unicode,
-                                                chr(unicode_decimal))
-                    index = text.find('&#')
-                text = text.replace("&amp;#160", " ")
-                text = text.replace("&amp;amp;", "&")
-                text = text.replace("&amp;gt;", ">")
-
-                label_train.append(int(lin['hyperpartisan']))
-
-                tokens = tokenizer(text, padding='max_length',
-                                   max_length=512, truncation='longest_first')
-                input_ids = torch.Tensor(tokens['input_ids'])
-                token_type_ids = torch.Tensor(tokens['token_type_ids'])
-                attention_mask = torch.Tensor(tokens['attention_mask'])
-
-                nsp_data = torch.stack(
-                    [input_ids, token_type_ids, attention_mask], dim=0)
-                input_train.append(nsp_data)
-
-                if i == 20:
-                    break
-
-            label_train = torch.Tensor(label_train)
-            input_train = torch.stack(input_train)
-
-            label_test = []
-            input_test = []
-            for i, lin in enumerate(hyperpartisan['validation']):
-                text = re.sub('<[^>]+>', '', lin['text'])
-                index = text.find('&#')
-                text = text.replace("..", "")
-                while index > -1:
-                    idx_semicolon = index + 2
-                    if index + 2 < len(text) and text[index+2].isdigit():
-                        while idx_semicolon < len(text) and text[idx_semicolon] != ';':
-                            idx_semicolon += 1
-                            if index - idx_semicolon > 6:
-                                break
-                        if text[index+2:idx_semicolon].isdigit():
-                            unicode_decimal = int(text[index+2:idx_semicolon])
-                            unicode = f'&#%d;' % unicode_decimal
-                            text = text.replace(unicode,
-                                                chr(unicode_decimal))
-                    index = text.find('&#')
-                text = text.replace("&amp;#160", " ")
-                text = text.replace("&amp;amp;", "&")
-                text = text.replace("&amp;gt;", ">")
-
-                label_test.append(int(lin['hyperpartisan']))
-                tokens = tokenizer(text, padding='max_length',
-                                   max_length=512, truncation='longest_first')
-                input_ids = torch.Tensor(tokens['input_ids'])
-                token_type_ids = torch.Tensor(tokens['token_type_ids'])
-                attention_mask = torch.Tensor(tokens['attention_mask'])
-
-                nsp_data = torch.stack(
-                    [input_ids, token_type_ids, attention_mask], dim=0)
-                input_test.append(nsp_data)
-
-                if i == 5:
-                    break
-
-            label_test = torch.Tensor(label_test)
-            input_test = torch.stack(input_test)
-
-            train_datasets.append(make_classification_dataset(
-                TensorDataset(input_train, label_train), task_labels=task_label))
-            test_datasets.append(make_classification_dataset(
-                TensorDataset(input_test, label_test), task_labels=task_label))
-
-        elif dataset == 'pubmed':
-            str_to_idx = {'INHIBITOR': 0,
-                          'ANTAGONIST': 1,
-                          'AGONIST': 2,
-                          'DOWNREGULATOR': 3,
-                          'PRODUCT-OF': 4,
-                          'SUBSTRATE': 5,
-                          'INDIRECT-UPREGULATOR': 6,
-                          'UPREGULATOR': 7,
-                          'INDIRECT-DOWNREGULATOR': 8,
-                          'ACTIVATOR': 9,
-                          'AGONIST-ACTIVATOR': 10,
-                          'AGONIST-INHIBITOR': 11,
-                          'SUBSTRATE_PRODUCT-OF': 12
-                          }
-            label_train = []
-            input_train = []
-            with jsonlines.open('../data/endtask/chemprot/train.txt') as f:
-                for lin in f:
-                    label_train.append(str_to_idx[lin['label']])
-
-                    tokens = tokenizer(lin['text'], padding='max_length',
-                                       max_length=512, truncation='longest_first')
-                    input_ids = torch.Tensor(tokens['input_ids'])
-                    token_type_ids = torch.Tensor(tokens['token_type_ids'])
-                    attention_mask = torch.Tensor(tokens['attention_mask'])
-
-                    nsp_data = torch.stack(
-                        [input_ids, token_type_ids, attention_mask], dim=0)
-                    input_train.append(nsp_data)
-            with jsonlines.open('../data/endtask/chemprot/dev.txt') as f:
-                for lin in f:
-                    label_train.append(str_to_idx[lin['label']])
-
-                    tokens = tokenizer(lin['text'], padding='max_length',
-                                       max_length=512, truncation='longest_first')
-                    input_ids = torch.Tensor(tokens['input_ids'])
-                    token_type_ids = torch.Tensor(tokens['token_type_ids'])
-                    attention_mask = torch.Tensor(tokens['attention_mask'])
-
-                    nsp_data = torch.stack(
-                        [input_ids, token_type_ids, attention_mask], dim=0)
-                    input_train.append(nsp_data)
-
-            label_train = torch.Tensor(label_train)
-            input_train = torch.stack(input_train)
-
-            label_test = []
-            input_test = []
-
-            with jsonlines.open('../data/endtask/chemprot/test.txt') as f:
-                for lin in f:
-                    label_test.append(str_to_idx[lin['label']])
-
-                    tokens = tokenizer(lin['text'], padding='max_length',
-                                       max_length=512, truncation='longest_first')
-                    input_ids = torch.Tensor(tokens['input_ids'])
-                    token_type_ids = torch.Tensor(tokens['token_type_ids'])
-                    attention_mask = torch.Tensor(tokens['attention_mask'])
-
-                    nsp_data = torch.stack(
-                        [input_ids, token_type_ids, attention_mask], dim=0)
+            if len(dir) == 0:
+                hyperpartisan_train = load_dataset('hyperpartisan_news_detection', 'bypublisher',
+                                                   trust_remote_code=True, split='train')
+                hyperpartisan_test = load_dataset('hyperpartisan_news_detection', 'bypublisher',
+                                                  trust_remote_code=True, split='validation[0%:50%]')
+                hyperpartisan_val = load_dataset('hyperpartisan_news_detection', 'bypublisher',
+                                                 trust_remote_code=True, split='validation[50%:100%]')
+
+                label_test = []
+                input_test = []
+                for lin in tqdm(hyperpartisan_test):
+                    label_test.append(int(lin['hyperpartisan']))
+                    nsp_data = preprocess_hyperpartisan(lin, tokenizer)
                     input_test.append(nsp_data)
-
-            label_test = torch.Tensor(label_test)
-            input_test = torch.stack(input_test)
-
-            train_datasets.append(make_classification_dataset(
-                TensorDataset(input_train, label_train), task_labels=task_label))
-            test_datasets.append(make_classification_dataset(
-                TensorDataset(input_test, label_test), task_labels=task_label))
-
-        elif dataset == 's2orc':
-            str_to_idx = {'Background': 0,
-                          'Uses': 1,
-                          'CompareOrContrast': 2,
-                          'Extends': 3,
-                          'Motivation': 4,
-                          'Future': 5
-                          }
-            label_train = []
-            input_train = []
-            with jsonlines.open('../data/endtask/citation_intent/train.txt') as f:
-                for lin in f:
-                    label_train.append(str_to_idx[lin['label']])
-
-                    tokens = tokenizer(lin['text'], padding='max_length',
-                                       max_length=512, truncation='longest_first')
-                    input_ids = torch.Tensor(tokens['input_ids'])
-                    token_type_ids = torch.Tensor(tokens['token_type_ids'])
-                    attention_mask = torch.Tensor(tokens['attention_mask'])
-
-                    nsp_data = torch.stack(
-                        [input_ids, token_type_ids, attention_mask], dim=0)
-                    input_train.append(nsp_data)
-            with jsonlines.open('../data/endtask/citation_intent/dev.txt') as f:
-                for lin in f:
-                    label_train.append(str_to_idx[lin['label']])
-
-                    tokens = tokenizer(lin['text'], padding='max_length',
-                                       max_length=512, truncation='longest_first')
-                    input_ids = torch.Tensor(tokens['input_ids'])
-                    token_type_ids = torch.Tensor(tokens['token_type_ids'])
-                    attention_mask = torch.Tensor(tokens['attention_mask'])
-
-                    nsp_data = torch.stack(
-                        [input_ids, token_type_ids, attention_mask], dim=0)
+                label_test = torch.Tensor(label_test)
+                input_test = torch.stack(input_test)
+
+                label_train = []
+                input_train = []
+                for lin in tqdm(hyperpartisan_train):
+                    label_train.append(int(lin['hyperpartisan']))
+                    nsp_data = preprocess_hyperpartisan(lin, tokenizer)
                     input_train.append(nsp_data)
+                label_train = torch.Tensor(label_train)
+                input_train = torch.stack(input_train)
+
+                label_val = []
+                input_val = []
+                for lin in tqdm(hyperpartisan_val):
+                    label_val.append(int(lin['hyperpartisan']))
+                    nsp_data = preprocess_hyperpartisan(lin, tokenizer)
+                    input_val.append(nsp_data)
+                label_val = torch.Tensor(label_val)
+                input_val = torch.stack(input_val)
+
+                train_dataset = make_classification_dataset(
+                    TensorDataset(input_train, label_train), task_labels=task_label)
+                test_dataset = make_classification_dataset(
+                    TensorDataset(input_test, label_test), task_labels=task_label)
+                val_dataset = make_classification_dataset(
+                    TensorDataset(input_val, label_val), task_labels=task_label)
+
+                torch.save(train_dataset, "%s/train.pt" % (path))
+                torch.save(val_dataset, "%s/val.pt" % (path))
+                torch.save(test_dataset, "%s/test.pt" % (path))
 
-            label_train = torch.Tensor(label_train)
-            input_train = torch.stack(input_train)
-
-            label_test = []
-            input_test = []
-            with jsonlines.open('../data/endtask/citation_intent/test.txt') as f:
-                for lin in f:
-                    label_test.append(str_to_idx[lin['label']])
+            else:
+                train_dataset = torch.load("%s/train.pt" % (path))
+                val_dataset = torch.load("%s/val.pt" % (path))
+                test_dataset = torch.load("%s/test.pt" % (path))
 
-                    tokens = tokenizer(lin['text'], padding='max_length',
-                                       max_length=512, truncation='longest_first')
-                    input_ids = torch.Tensor(tokens['input_ids'])
-                    token_type_ids = torch.Tensor(tokens['token_type_ids'])
-                    attention_mask = torch.Tensor(tokens['attention_mask'])
+        elif dataset == 'pubmed':
+            if len(dir) == 0:
+                str_to_idx = {'INHIBITOR': 0,
+                              'ANTAGONIST': 1,
+                              'AGONIST': 2,
+                              'DOWNREGULATOR': 3,
+                              'PRODUCT-OF': 4,
+                              'SUBSTRATE': 5,
+                              'INDIRECT-UPREGULATOR': 6,
+                              'UPREGULATOR': 7,
+                              'INDIRECT-DOWNREGULATOR': 8,
+                              'ACTIVATOR': 9,
+                              'AGONIST-ACTIVATOR': 10,
+                              'AGONIST-INHIBITOR': 11,
+                              'SUBSTRATE_PRODUCT-OF': 12
+                              }
+                label_train = []
+                input_train = []
+                with jsonlines.open('../data/endtask/chemprot/train.txt') as f:
+                    for lin in tqdm(f):
+                        label_train.append(str_to_idx[lin['label']])
+                        nsp_data = preprocess(lin, tokenizer)
+                        input_train.append(nsp_data)
+                label_train = torch.Tensor(label_train)
+                input_train = torch.stack(input_train)
+
+                label_val = []
+                input_val = []
+                with jsonlines.open('../data/endtask/chemprot/dev.txt') as f:
+                    for lin in tqdm(f):
+                        label_val.append(str_to_idx[lin['label']])
+                        nsp_data = preprocess(lin, tokenizer)
+                        input_val.append(nsp_data)
+                label_val = torch.Tensor(label_val)
+                input_val = torch.stack(input_val)
+
+                label_test = []
+                input_test = []
+                with jsonlines.open('../data/endtask/chemprot/test.txt') as f:
+                    for lin in tqdm(f):
+                        label_test.append(str_to_idx[lin['label']])
+                        nsp_data = preprocess(lin, tokenizer)
+                        input_test.append(nsp_data)
+                label_test = torch.Tensor(label_test)
+                input_test = torch.stack(input_test)
+
+                train_dataset = make_classification_dataset(
+                    TensorDataset(input_train, label_train), task_labels=task_label)
+                test_dataset = make_classification_dataset(
+                    TensorDataset(input_test, label_test), task_labels=task_label)
+                val_dataset = make_classification_dataset(
+                    TensorDataset(input_val, label_val), task_labels=task_label)
+
+                torch.save(train_dataset, "%s/train.pt" % (path))
+                torch.save(val_dataset, "%s/val.pt" % (path))
+                torch.save(test_dataset, "%s/test.pt" % (path))
 
-                    nsp_data = torch.stack(
-                        [input_ids, token_type_ids, attention_mask], dim=0)
-                    input_test.append(nsp_data)
+            else:
+                train_dataset = torch.load("%s/train.pt" % (path))
+                val_dataset = torch.load("%s/val.pt" % (path))
+                test_dataset = torch.load("%s/test.pt" % (path))
 
-            label_test = torch.Tensor(label_test)
-            input_test = torch.stack(input_test)
+        elif dataset == 's2orc':
+            if len(dir) == 0:
+                str_to_idx = {'Background': 0,
+                              'Uses': 1,
+                              'CompareOrContrast': 2,
+                              'Extends': 3,
+                              'Motivation': 4,
+                              'Future': 5
+                              }
+                label_train = []
+                input_train = []
+                with jsonlines.open('../data/endtask/citation_intent/train.txt') as f:
+                    for lin in tqdm(f):
+                        label_train.append(str_to_idx[lin['label']])
+                        nsp_data = preprocess(lin, tokenizer)
+                        input_train.append(nsp_data)
+                label_train = torch.Tensor(label_train)
+                input_train = torch.stack(input_train)
+
+                label_val = []
+                input_val = []
+                with jsonlines.open('../data/endtask/citation_intent/dev.txt') as f:
+                    for lin in tqdm(f):
+                        label_val.append(str_to_idx[lin['label']])
+                        nsp_data = preprocess(lin, tokenizer)
+                        input_val.append(nsp_data)
+                label_val = torch.Tensor(label_val)
+                input_val = torch.stack(input_val)
+
+                label_test = []
+                input_test = []
+                with jsonlines.open('../data/endtask/citation_intent/test.txt') as f:
+                    for lin in tqdm(f):
+                        label_test.append(str_to_idx[lin['label']])
+                        nsp_data = preprocess(lin, tokenizer)
+                        input_test.append(nsp_data)
+                label_test = torch.Tensor(label_test)
+                input_test = torch.stack(input_test)
+
+                train_dataset = make_classification_dataset(
+                    TensorDataset(input_train, label_train), task_labels=task_label)
+                test_dataset = make_classification_dataset(
+                    TensorDataset(input_test, label_test), task_labels=task_label)
+                val_dataset = make_classification_dataset(
+                    TensorDataset(input_val, label_val), task_labels=task_label)
+
+                torch.save(train_dataset, "%s/train.pt" % (path))
+                torch.save(val_dataset, "%s/val.pt" % (path))
+                torch.save(test_dataset, "%s/test.pt" % (path))
 
-            train_datasets.append(make_classification_dataset(
-                TensorDataset(input_train, label_train), task_labels=task_label))
-            test_datasets.append(make_classification_dataset(
-                TensorDataset(input_test, label_test), task_labels=task_label))
+            else:
+                train_dataset = torch.load("%s/train.pt" % (path))
+                val_dataset = torch.load("%s/val.pt" % (path))
+                test_dataset = torch.load("%s/test.pt" % (path))
 
-    return dataset_benchmark(train_datasets, test_datasets)
+    return dataset_benchmark(train_datasets, test_datasets, other_streams_datasets={"val": val_datasets})
diff --git a/src/modnets/__pycache__/bert.cpython-310.pyc b/src/modnets/__pycache__/bert.cpython-310.pyc
index 2ac5771..ac0adfa 100644
Binary files a/src/modnets/__pycache__/bert.cpython-310.pyc and b/src/modnets/__pycache__/bert.cpython-310.pyc differ
diff --git a/src/modnets/bert.py b/src/modnets/bert.py
index 4b2582a..f15f3fb 100644
--- a/src/modnets/bert.py
+++ b/src/modnets/bert.py
@@ -165,7 +165,6 @@ class BertSelfAttention(am.MultiTaskModule):
         past_key_value: Optional[Tuple[Tuple[torch.FloatTensor]]] = None,
         output_attentions: Optional[bool] = False,
     ) -> Tuple[torch.Tensor]:
-        print(hidden_states, task_label)
         mixed_query_layer = self.query(hidden_states, task_label)
 
         # If this is instantiated as a cross-attention module, the keys
@@ -202,7 +201,6 @@ class BertSelfAttention(am.MultiTaskModule):
         use_cache = past_key_value is not None
         if self.is_decoder:
             past_key_value = (key_layer, value_layer)
-
         # Take the dot product between "query" and "key" to get the raw attention scores.
         attention_scores = torch.matmul(
             query_layer, key_layer.transpose(-1, -2))
@@ -212,7 +210,6 @@ class BertSelfAttention(am.MultiTaskModule):
         if attention_mask is not None:
             # Apply the attention mask is (precomputed for all layers in BertModel forward() function)
             attention_scores = attention_scores + attention_mask
-        print(attention_scores.shape, attention_mask)
 
         # Normalize the attention scores to probabilities.
         attention_probs = nn.functional.softmax(attention_scores, dim=-1)
@@ -406,6 +403,7 @@ class BertLayer(am.MultiTaskModule):
         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2
         self_attn_past_key_value = past_key_value[:
                                                   2] if past_key_value is not None else None
+
         self_attention_outputs = self.attention(
             hidden_states,
             attention_mask,
@@ -482,7 +480,6 @@ class BertEncoder(am.MultiTaskModule):
 
             layer_head_mask = head_mask[i] if head_mask is not None else None
             past_key_value = past_key_values[i] if past_key_values is not None else None
-
             layer_outputs = layer_module(
                 hidden_states,
                 attention_mask,
@@ -493,7 +490,6 @@ class BertEncoder(am.MultiTaskModule):
                 past_key_value,
                 output_attentions,
             )
-
             hidden_states = layer_outputs[0]
             if use_cache:
                 next_decoder_cache += (layer_outputs[-1],)
@@ -757,7 +753,6 @@ class BertModel(am.MultiTaskModule):
             inputs_embeds=inputs_embeds,
             past_key_values_length=past_key_values_length,
         )
-
         encoder_outputs = self.encoder(
             embedding_output,
             attention_mask=extended_attention_mask,
@@ -829,7 +824,6 @@ class BertForPreTraining(am.MultiTaskModule):
         output_hidden_states: Optional[bool] = None,
         return_dict: Optional[bool] = None,
     ):
-        print("!!!!", task_label)
         outputs = self.bert(
             input_ids,
             attention_mask=attention_mask,
diff --git a/src/temp.py b/src/temp.py
index 394c161..6a73d40 100644
--- a/src/temp.py
+++ b/src/temp.py
@@ -21,13 +21,16 @@ from datasets import Dataset
 from torch.utils.data import TensorDataset
 from dataset import create_unlabeled_benchmark, create_endtask_benchmark
 from typing import Any, Callable, Dict, Iterable, List, NamedTuple, Optional, Sequence, Union
-
+import os
+os.environ['CUDA_LAUNCH_BLOCKING'] = "1"
+os.environ["CUDA_VISIBLE_DEVICES"] = "0"
 FLAGS = argparse.ArgumentParser()
 FLAGS.add_argument('--datasets', type=str, default='s2orc',
                    help='Names of datasets')
 FLAGS.add_argument('--model', type=str, default='bert-base-uncased',
                    help='type of pretrained model')
-FLAGS.add_argument('--epoch', type=int, default=1)
+FLAGS.add_argument('--epoch', type=int, default=5)
+FLAGS.add_argument('--seed', type=int, default=1)
 args = FLAGS.parse_args()
 
 
@@ -40,15 +43,24 @@ class PiggybackStrategy(SupervisedTemplate):
         return super().train(experiences, eval_streams, **kwargs)
 
     def forward(self):
-        # input_ids = d[0, :]
-        # token_type_ids = d[1, :]
-        # attention_mask = d[2, :]
-        input_ids = self.mb_x[:, 0, :].type(torch.IntTensor).to('cuda')
-        token_type_ids = self.mb_x[:, 1, :].type(torch.IntTensor).to('cuda')
-        attention_mask = self.mb_x[:, 2, :].type(torch.IntTensor).to('cuda')
-        print(input_ids.shape, token_type_ids.shape,
-              attention_mask.shape, self.mb_task_id)
-        return self.model(input_ids, token_type_ids, attention_mask, self.mb_task_id)
+        self.input_ids = self.mb_x[:, 0, :].type(torch.IntTensor).to('cuda')
+        self.original_input_ids = self.mb_x[:, 1, :].type(torch.IntTensor).to(
+            'cuda')
+        self.token_type_ids = self.mb_x[:, 2, :].type(torch.IntTensor).to(
+            'cuda')
+        self.attention_mask = self.mb_x[:, 3, :].type(torch.IntTensor).to(
+            'cuda')
+        task_label = self.mb_task_id[0].item()
+        return self.model(input_ids=self.input_ids, token_type_ids=self.token_type_ids, attention_mask=self.attention_mask, task_label=task_label)
+
+    def criterion(self):
+        prediction_scores, seq_relationship_score = self.mb_output
+        masked_token = self.input_ids.eq(103)
+        loss_mask = self._criterion(
+            prediction_scores[masked_token], self.original_input_ids[masked_token].type(torch.LongTensor).to('cuda'))
+        loss_nsp = self._criterion(
+            seq_relationship_score, self.mb_y.type(torch.LongTensor).to('cuda'))
+        return loss_mask + loss_nsp
 
     def backward(self):
         super().backward()
@@ -81,6 +93,36 @@ class PiggybackStrategy(SupervisedTemplate):
                                 abs_in_proj_weights.mean())
 
 
+def copy_weights(model, model_pretrained):
+    # Copy weights of pretrained model
+    module_list = list(model_pretrained.modules())
+    i = 0
+
+    for module in model.modules():
+        if i == len(module_list):
+            break
+
+        if 'ElementWiseLinear' in str(type(module)):
+            module.weight.data.copy_(module_list[i].weight.data)
+            module.bias.data.copy_(module_list[i].bias.data)
+
+        elif 'Embedding' in str(type(module)) and 'Bert' not in str(type(module)):
+            module.weight.data.copy_(module_list[i].weight.data)
+
+        elif 'LayerNorm' in str(type(module)):
+            module.weight.data.copy_(module_list[i].weight.data)
+            module.bias.data.copy_(module_list[i].bias.data)
+            module.eval()
+
+        elif 'Dict' in str(type(module)) or 'BertForPreTraining' in str(type(module)):
+            continue
+        i += 1
+
+
+random.seed(args.seed)
+torch.manual_seed(args.seed)
+torch.cuda.manual_seed_all(args.seed)
+
 tokenizer = AutoTokenizer.from_pretrained(args.model)
 model_pretrained = BertModel.from_pretrained(args.model)
 
@@ -92,18 +134,13 @@ criterion = nn.CrossEntropyLoss()
 optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
 
 interactive_logger = InteractiveLogger()
-# wandb_logger = WandBLogger(
-#     project_name="piggyback_ViT",
-#     run_name="piggyback_%s" % (args.datasets),
-#     path="../checkpoint",
-#     config=vars(args)
-# )
+wandb_logger = WandBLogger(
+    project_name="piggyback_ViT",
+    run_name="piggyback_%s" % (args.datasets),
+    path="../checkpoint",
+    config=vars(args)
+)
 eval_plugin = EvaluationPlugin(
-    amet.accuracy_metrics(
-        epoch=True,
-        experience=True,
-        stream=True
-    ),
     amet.loss_metrics(
         epoch=True,
         experience=True,
@@ -114,9 +151,9 @@ eval_plugin = EvaluationPlugin(
 cl_strategy = PiggybackStrategy(model,
                                 optimizer,
                                 criterion,
-                                train_mb_size=128,
+                                train_mb_size=16,
                                 train_epochs=args.epoch,
-                                eval_mb_size=128,
+                                eval_mb_size=16,
                                 device='cuda',
                                 evaluator=eval_plugin,
                                 eval_every=1)
@@ -129,20 +166,12 @@ benchmark_unlabeled = create_unlabeled_benchmark(args, tokenizer)
 train_stream = benchmark_unlabeled.train_stream
 test_stream = benchmark_unlabeled.test_stream
 
+copy_weights(model, model_pretrained)
 torch.set_printoptions(profile="full")
 
 results = []
 for train_exp, test_exp in zip(train_stream, test_stream):
-    # d = train_exp.dataset[0][0]
-    # task_label = train_exp.dataset[0][2]
-    # input_ids = d[0, :]
-    # token_type_ids = d[1, :]
-    # attention_mask = d[2, :]
-    # a, b = model(input_ids=input_ids[None, :].type(torch.IntTensor), token_type_ids=token_type_ids[None, :].type(torch.IntTensor),
-    #              attention_mask=attention_mask[None, :].type(torch.IntTensor), task_label=task_label)
-    # print(a.shape, b.shape)
-    # model.adaptation(train_exp)
-    print(model)
+    print(len(train_exp.dataset))
     print("Start of experience: ", train_exp.current_experience)
     print("Current Classes: ", train_exp.classes_in_this_experience)
 
@@ -155,7 +184,3 @@ for train_exp, test_exp in zip(train_stream, test_stream):
     result = cl_strategy.eval(test_stream)
 
     results.append(result)
-# print(model)
-
-
-# self.mbatch
